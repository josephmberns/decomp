# -*- coding: utf-8 -*-
"""Trainer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PhPQcpu9LlZ-XQxNdXgUTpDnKYtLRBSe
"""

import platform
import torch
from datasets import load_dataset, DatasetDict, Dataset
from transformers import LlamaForCausalLM, TrainingArguments, Trainer, CodeLlamaTokenizer ,BitsAndBytesConfig, TrainerCallback, AutoTokenizer, AutoModelForCausalLM
import os
import csv
import evaluate
import sys
import random
import torch
import matplotlib.pyplot as plt
import pickle
from tqdm import tqdm
from torch.utils.data import DataLoader # Import DataLoader
import math
from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction
from nltk.metrics.distance import edit_distance
from peft import get_peft_model, PeftModel, LoraConfig
from huggingface_hub import notebook_login, login

source = "source"
decompiled = "decompiled"
from_pretrained = False

def print_system_specs():
    # Checking the PyTorch version
    print("Your Python version is " + platform.python_version())
    print("Your PyTorch version is " + torch.__version__)

    # Check if CUDA is available
    is_cuda_available = torch.cuda.is_available()
    print("CUDA Available:", is_cuda_available)

    # Get the number of available CUDA devices
    num_cuda_devices = torch.cuda.device_count()
    print("Number of CUDA devices:", num_cuda_devices)
    if is_cuda_available:
        for i in range(num_cuda_devices):
            # Get CUDA device properties
            device = torch.device('cuda', i)
            print(f"--- CUDA Device {i} ---")
            print("Name:", torch.cuda.get_device_name(i))
            print("Compute Capability:", torch.cuda.get_device_capability(i))
            print("Total Memory:", torch.cuda.get_device_properties(i).total_memory, "bytes")
    # Get CPU information
    print("--- CPU Information ---")
    print("Processor:", platform.processor())
    print("System:", platform.system(), platform.release())
    print("Python Version:", platform.python_version())

def get_device():
  print_system_specs()
  if torch.cuda.is_available():
      device = torch.device("cuda")
      print("There are %d GPU(s) available." % torch.cuda.device_count())
      print("We will use the GPU:", torch.cuda.get_device_name(0))
  else:
      device = torch.device("cpu")
      print("No GPU available, using the CPU instead.")
  return device

def load_tokenizer(model_name="meta-llama/Llama-2-7b-hf"):
  if from_pretrained:
     tokenizer = AutoTokenizer.from_pretrained("./llama-finetuned")
  else:
    tokenizer = CodeLlamaTokenizer.from_pretrained(model_name)
  tokenizer.pad_token = tokenizer.eos_token
  tokenizer.padding_side = 'left'
  return tokenizer

def format_prompt(max_tokenized_length, input_program, source_program = ""):
  eval_prompt = f"You are a powerful decompiler model. Your job is to convert the segment of ะก code decompiled into a more human-readable form. That is, you should change the names of variables and functions and delete unnecessary parts of the code so that it looks more like the source code of a C program. You are given a ะก code decompiled using RetDec decompiler. You must output the source code of a C program. ### Your input: "
  ending_prompt = f" ### The original C program: "
  input_len = int((max_tokenized_length - len(eval_prompt) - len(ending_prompt)) / 2)
  if len(input_program) > input_len:
    eval_prompt += input_program[0:input_len]
  else:
    eval_prompt += input_program
     
  eval_prompt += ending_prompt

  if len(source_program) > input_len:
    eval_prompt += source_program[0:input_len]
  else:
    eval_prompt += source_program

  return eval_prompt

def read_rows(file_name):
  data = []
  with open(file_name) as csvfile:
      spamreader = csv.reader(csvfile, delimiter=",", quotechar='`')
      for row in spamreader:
        if " " in row[0]:
          data.append({source: row[0], decompiled: row[1]})
  return data

def read_rows_dictionary(file_name):
  train_data = { source : [], decompiled: []}
  with open(file_name) as csvfile:
      spamreader = csv.reader(csvfile, delimiter=",", quotechar='`')
      for row in spamreader:
        if " " in row[0]:
          train_data[source].append(row[0])
          train_data[decompiled].append(row[1])
  return train_data

def write_rows(file_name, data):
  with open(file_name, 'w') as csvfile:
    writer = csv.writer(csvfile, quotechar='`')
    for row in data:
      writer.writerow([row[source], row[decompiled]])

def load_datasets(percent_testing, percent_validation):
  csv.field_size_limit(sys.maxsize)
  if not os.path.exists(f"data/train.csv"):
    ghidra_data = read_rows("data/decompiled_ghidra.csv")
    retdec_data = read_rows("data/decompiled_retdec.csv")
    hexrays_data = read_rows("data/decompiled_hexrays.csv")

    total = len(ghidra_data) + len(retdec_data) + len(hexrays_data)
    num_testing = math.floor(percent_testing * total)
    num_validation = math.floor(percent_validation * total)
    num_training = total - num_testing - num_validation

    retdec_testing_data = random.sample(retdec_data, math.floor(num_testing / 3))
    ghidra_testing_data = random.sample(ghidra_data, math.ceil(num_testing / 3))
    hexrays_testing_data = random.sample(hexrays_data, math.floor(num_testing / 3))
    # Remove selected elements from the original array
    remaining_elements = [item for item in retdec_data if item not in retdec_testing_data] + [item for item in ghidra_data if item not in ghidra_testing_data] + [item for item in hexrays_data if item not in hexrays_testing_data]
    print(f"Remaining elements after training data extracted: {len(remaining_elements)}")
    validation_data = random.sample(remaining_elements, num_validation)
    training_data = [item for item in remaining_elements if item not in validation_data]

    write_rows("data/train.csv", training_data)
    write_rows("data/validation.csv", validation_data)
    write_rows("data/test_ghidra.csv", ghidra_testing_data)
    write_rows("data/test_retdec.csv", retdec_testing_data)
    write_rows("data/test_hexrays.csv", hexrays_testing_data)

    train_data = {
      source: [item[source] for item in training_data],
      decompiled: [item[decompiled] for item in training_data]
    }

    validate_data = {
        source: [item[source] for item in validation_data],
        decompiled: [item[decompiled] for item in validation_data]
    }

    retdec_test_data = {
        source: [item[source] for item in retdec_testing_data],
        decompiled: [item[decompiled] for item in retdec_testing_data]
    }

    ghidra_test_data = {
        source: [item[source] for item in ghidra_testing_data],
        decompiled: [item[decompiled] for item in ghidra_testing_data]
    }

    hexrays_test_data = {
        source: [item[source] for item in hexrays_testing_data],
        decompiled: [item[decompiled] for item in hexrays_testing_data]
    }

  else:
    train_data = read_rows_dictionary("data/train.csv")
    validate_data = read_rows_dictionary("data/validation.csv")
    ghidra_test_data = read_rows_dictionary("data/test_ghidra.csv")
    retdec_test_data = read_rows_dictionary("data/test_retdec.csv")
    hexrays_test_data = read_rows_dictionary("data/test_hexrays.csv")

  print(f"Number of training examples: {len(train_data[source])}")
  print(f"Number of validation examples: {len(validate_data[source])}")
  print(f"Number of retdec testing examples: {len(retdec_test_data[source])}")
  print(f"Number of ghidra testing examples: {len(ghidra_test_data[source])}")
  print(f"Number of ghidra testing examples: {len(hexrays_test_data[source])}")


  # Combining the datasets into a DatasetDict
  dataset = {
      'train': Dataset.from_dict(train_data),
      'validation': Dataset.from_dict(validate_data),
      'ghidra_test': Dataset.from_dict(ghidra_test_data),
      'retdec_test': Dataset.from_dict(retdec_test_data),
      'hexrays_test': Dataset.from_dict(hexrays_test_data)
  }

  return DatasetDict(dataset)

def load_model(tokenizer, model_name="meta-llama/Llama-2-7b-hf"):
  # load the basic model
  model = LlamaForCausalLM.from_pretrained(
      model_name,
      torch_dtype=torch.float16,
      device_map="auto",
      quantization_config=BitsAndBytesConfig(
          load_in_4bit=True,
          bnb_4bit_compute_dtype=torch.float16,
          bnb_4bit_use_double_quant=True,
          bnb_4bit_quant_type='nf4',
      )
  )  
  if from_pretrained:
    model = PeftModel.from_pretrained(model, "./llama-finetuned")
  else:
    adapter = LoraConfig(
      r=4,           # Rank of the LoRA update matrices
      lora_alpha=16,  # Scaling factor
      target_modules=["q_proj", "v_proj", "k_proj", "out_proj"],  # Modules to apply LoRA (depends on the model architecture)
      lora_dropout=0.2,  # Dropout for LoRA updates
      bias="none",     # Add bias or not
      task_type="CAUSAL_LM"  # Task type: language modeling
    )

    model.add_adapter(adapter)

  return model

def train(model, tokenizer, training_args, tokenized_datasets, id, save=True):
  """
  Trains the model using the provided training arguments and dataset.

  Args:
    model: The model to train.
    tokenizer: The tokenizer used for the model.
    training_args: The training arguments.
    tokenized_datasets: The tokenized dataset.
    save: Whether to save the model after training.
  """

  class LossTrackerCallback(TrainerCallback):
      def __init__(self):
          self.losses = []

      def on_log(self, args, state, control, logs=None, **kwargs):
          if logs and "loss" in logs:
              self.losses.append(logs["loss"])

  loss_tracker = LossTrackerCallback()

  # Define optimizer and learning rate scheduler
  trainer = Trainer(
    model, 
    training_args, 
    train_dataset=tokenized_datasets["train"], 
    eval_dataset=tokenized_datasets["validation"], 
    tokenizer=tokenizer, callbacks=[loss_tracker]
  )
  trainer.train()

  plt.plot(loss_tracker.losses)
  plt.xlabel("Training Sample #")
  plt.ylabel("Loss")
  plt.savefig(f"losses_{str(id)}.png")
  if save:
    # save model and tokens
    model.save_pretrained("./llama-finetuned")
    tokenizer.save_pretrained("./llama-finetuned")

def test_individual(model, tokenizer, input_text, device):
  input_ids = input_text['input_ids'].to(device)
  attention_mask = input_text["attention_mask"].to(device)
  outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_new_tokens=1000)
  output = list(map(lambda out: tokenizer.decode(out, skip_special_tokens=True), outputs))
  return output

def split_response(pred):
  j = 0
  while (pred[j]+pred[j+1]+pred[j+2]+pred[j+3]+pred[j+4])!='### T':
      j+=1
  pred=pred[j::]
  pred = pred[39::]

  return pred

def test(model, tokenizer, dataset, device):
  references = []
  hypotheses = []
  test_dataloader = DataLoader(dataset, shuffle=True, batch_size=5)
  model.eval()
  for id, test in enumerate(test_dataloader):
    if id % 10 == 0:
      print(f"Testing batch_num {id}")
    output = test_individual(model, tokenizer, test, device)
    for id, out in enumerate(output):
      if "program:" in out:
        ref = split_response(test['actual_labels'][id])
        hypothesised = split_response(out)
        references.append(ref)
        hypotheses.append(hypothesised)
  return references, hypotheses

def evaluate_results(references, hypotheses):
  print(f"Training on {len(references)} samples")
  # Sentence BLEU
  total_sentence_bleu = 0
  smooth = SmoothingFunction().method1 # Smoothing is used for short sentences
  for pred, ref in zip(hypotheses, references):
    total_sentence_bleu += sentence_bleu(ref, pred, smoothing_function=smooth)
  print(f"Total Sentence BLEU: {total_sentence_bleu}")
  average_sentence_bleu = total_sentence_bleu / len(references)
  print(f"Average Sentence BLEU Score: {average_sentence_bleu:.4f}")

  # Corpus BLEU
  bleu_score = corpus_bleu(references, hypotheses)
  print(f"Corpus BLEU Score: {bleu_score:.4f}")

  # AED
  total_distance = 0
  for pred, ref in zip(hypotheses, references):
      total_distance += edit_distance(pred, ref)
  aed = total_distance / len(hypotheses)

  print(f"Average Edit Distance (AED): {aed:.4f}")

def run(training_args, device, model_name, max_tokenized_length, save_to_file, percent_testing, percent_validation, id):
  dataset = load_datasets(percent_testing, percent_validation)
  print("\n\nLoading Tokenizer....\n")
  tokenizer = load_tokenizer(model_name)
  torch.cuda.empty_cache()

  def preprocess_function(examples):
    prompts = [format_prompt(max_tokenized_length, example) for example in examples[decompiled]]
    output = tokenizer(prompts, truncation=True, padding="max_length", max_length=max_tokenized_length, return_tensors="pt")

    labels = []
    for i, example in enumerate(examples[source]):
      decomp = examples[decompiled][i]
      labels.append(format_prompt(max_tokenized_length, decomp, example))

    label_outputs = tokenizer(labels, truncation=True, padding="max_length", max_length=max_tokenized_length, return_tensors="pt")
    output['labels'] = label_outputs['input_ids']
    output['actual_labels'] = labels
    return output

  # Apply preprocessing
  print("\n\nTokenizing Dataset....\n")
  tokenized_dataset = dataset.map(preprocess_function, batched=True)
  tokenized_dataset = tokenized_dataset.remove_columns(["decompiled", "source"])
  tokenized_dataset.set_format("torch")
  torch.cuda.empty_cache()

  print("\n\nLoading Model....\n")
  model = load_model(tokenizer, model_name).to(device)
  torch.cuda.empty_cache()

  print("Training Model....\n")
  train(model, tokenizer, training_args, tokenized_dataset, id, save_to_file)
  torch.cuda.empty_cache()

  print("\n\nTesting Model On Ghidra....\n")
  references, hypotheses = test(model, tokenizer, tokenized_dataset["ghidra_test"], device)

  # Create an array
  with open(f'references_ghidra_{str(id)}.pkl', 'wb') as f:
      pickle.dump(references, f)

  with open(f'hypotheses_ghidra_{str(id)}.pkl', 'wb') as f:
      pickle.dump(hypotheses, f)

  print("\n\nEvaluating...\n")
  evaluate_results(references, hypotheses)
  torch.cuda.empty_cache()

  print("\n\nTesting Model On Retdec....\n")
  references, hypotheses = test(model, tokenizer, tokenized_dataset["retdec_test"], device)

  # Create an array
  with open(f'references_retdec_{str(id)}.pkl', 'wb') as f:
      pickle.dump(references, f)

  with open(f'hypotheses_retdec_{str(id)}.pkl', 'wb') as f:
      pickle.dump(hypotheses, f)

  print("\n\nEvaluating...\n")
  evaluate_results(references, hypotheses)
  torch.cuda.empty_cache()

  print("\n\nTesting Model On HexRays....\n")
  references, hypotheses = test(model, tokenizer, tokenized_dataset["hexrays_test"], device)

  # Create an array
  with open(f'references_hexrays_{str(id)}.pkl', 'wb') as f:
      pickle.dump(references, f)

  with open(f'hypotheses_hexrays_{str(id)}.pkl', 'wb') as f:
      pickle.dump(hypotheses, f)

  print("\n\nEvaluating...\n")
  evaluate_results(references, hypotheses)

  return references, hypotheses

def main():
  # HYPERPARAMTERS ----------------------
  PER_DEVICE_TRAIN_BATCH_SIZE = 2
  PER_DEVICE_EVAL_BATCH_SIZE = 2
  NUM_TRAIN_EPOCHS = 1
  LEARNING_RATE = 1e-5

  MAX_TOKENIZED_LENGTH=1250
  SAVE_MODEL_TO_FILE=True
  PERCENT_VALIDATION = 0.1
  PERCENT_TESTING = 0.2
  id = 11

  model_name = "codellama/CodeLlama-7b-hf"

  # -------------------------------------

  training_args = TrainingArguments(
      output_dir=f"model-output-dir",
      evaluation_strategy="epoch",
      save_strategy="epoch",
      save_total_limit=1,
      dataloader_pin_memory=False,  # Avoid extra memory overhead
      fp16=True,  # Use mixed precision for speedup
      per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,
      per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,
      num_train_epochs=NUM_TRAIN_EPOCHS,
      learning_rate=LEARNING_RATE,
      # warmup_steps=NUM_WARM_UP_STEPS,
      save_steps=100,
      logging_dir="./logs",       # Directory for logging
      logging_steps=5,           # Log loss every 10 steps
      report_to="none"            # Prevent sending logs to third-party tools
  )
  torch.cuda.empty_cache()
  device = get_device()
  run(training_args, device, model_name, MAX_TOKENIZED_LENGTH, SAVE_MODEL_TO_FILE, PERCENT_TESTING, PERCENT_VALIDATION, id)

if __name__ == "__main__":
  login("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")
  main()

# Evaluating Model On Ghidra...

# Average Sentence BLEU Score: 0.0010
# Corpus BLEU Score: 0.0004
# Average Edit Distance (AED): 693.2978


# Evaluating Model On Retdec...

# len ref: 178
# len hyp: 178
# Total Sentence BLEU: 0.17582994558599363
# Average Sentence BLEU Score: 0.0010
# Corpus BLEU Score: 0.0004
# Average Edit Distance (AED): 692.8708